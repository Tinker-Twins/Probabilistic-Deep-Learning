\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Notes\_02\_Bayes\_by\_Backprop}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{bayes-by-backprop}{%
\section{Bayes by backprop}\label{bayes-by-backprop}}

    This is a review of the \emph{Bayes by backprop} method for quantifying
epistemic uncertainty in a neural network model.

    \hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Let's discuss one way to introduce weight uncertainty into neural
networks - the \emph{Bayes by Backprop} method introduced in this paper:

\begin{itemize}
\tightlist
\item
  Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra.
  \href{https://arxiv.org/pdf/1505.05424.pdf}{Weight uncertainty in
  neural networks}. In \emph{Proceedings of the 32nd International
  Conference on Machine Learning}.
\end{itemize}

The main idea is as follows. In a traditional neural network, as shown
in the left figure below, each weight has a single value. The true value
of this weight is not certain. A lot of this uncertainty comes from
imperfect training data, which does not exactly describe the
distribution the data samples were drawn from.

As an analogy, consider the problem of determining the population
average height by measuring 100 randomly selected people. An estimate of
the population mean is then the mean calculated across these 100.
However, this is just an estimate; we may, by chance, have selected 100
people that are slightly taller than expected, or shorter. Recall that
this is called \emph{epistemic} uncertainty, and we expect it to
decrease as the amount of training data increases. For example, the
uncertainty on an estimate obtained using 100 people is larger than one
using 10,000 people.

Ultimately, we want to include such uncertainty in deep learning models.
This is done by changing each weight from a single deterministic value
(i.e.~fixed point estimate) to a \emph{probability distribution}. We
then learn the parameters of this distribution. Consider a neural
network weight \(w_i\). In a standard (deterministic) neural network,
this has a single value \(\hat{w}_i\), learnt via backpropagation. In a
neural network with weight uncertainty, each weight is represented by a
probability distribution, and the \emph{parameters} of this distribution
are learned via backpropagation. Suppose, for example, that each weight
follows a normal distribution. This has two parameters: a mean \(\mu_i\)
and a standard deviation \(\sigma_i\). - Classic deterministic NN:
\(w_i = \hat{w}_i\) - NN with weight uncertainty represented by normal
distribution: \(w_i \sim N(\hat{\mu}_i, \hat{\sigma}_i)\).

An image of the situation, provided in the paper, is as follows.

Since the weights are uncertain, the feedforward value of some input
\(x_i\) is not constant. A single feedforward value is determined in two
steps: 1. Sample each network weight from their respective distributions
-- this gives a single set of network weights. 2. Use these weights to
determine a feedforward value \(\hat{y}_i\).

Hence, the key question is how to determine the parameters of the
distribution for each network weight. The paper introduces exactly such
a scheme, called \emph{Bayes by Backprop}. The details are discussed in
the remainder sections.

    \hypertarget{bayesian-learning}{%
\subsection{Bayesian learning}\label{bayesian-learning}}

\textbf{Note}: We use the notation \(P\) to refer to a probability
density. For simplicity, we'll only consider continuous distributions
(which have a density). In the case of discrete distributions, \(P\)
would represent a probability mass and integrals should be changed to
sums. However, the formulae are the same.

\emph{Bayesian} methods represent one common framework in which to
conduct statistical inference. We only provide a very short introduction
here, but for a more detailed account of Bayesian inference you could
read the
\href{https://en.wikipedia.org/wiki/Bayesian_inference}{Wikipedia
article} and references therein.

What you need to know now is that Bayesian methods can be used to
calculate the distribution of a model parameter given some data. In the
context of weight uncertainty in neural networks, this is convenient,
since we are looking for the distribution of weights (model parameters)
given some (training) data. The key step relies on Bayes' theorem. This
theorem states, in mathematical notation, that

\[ P(w | D) = \frac{P(D | w) P(w)}{\int P(D | w') P(w') \text{d}w'} \]

where the terms mean the following: - \(D\) is some data, e.g.~\(x\) and
\(y\) value pairs: \(D = \{(x_1, y_1), \ldots, (x_n, y_n)\}\). This is
sometimes called the \emph{evidence}. - \(w\) is the value of a model
weight. - \(P(w)\) is called the \emph{prior}. This is our ``prior''
belief on the probability density of a model weight, i.e.~the
distribution that we postulate before seeing any data. - \(P(D | w)\) is
the \emph{likelihood} of having observed data \(D\) given weight \(w\).
It is used to calculate the negative log-likelihood loss as the
oprimization criterion. - \(P(w | D)\) is the \emph{posterior} density
of the distribution of the model weight at value \(w\), given our
training data. It is called \emph{posterior} since it represents the
distribution of our model weight \emph{after} taking the training data
into account.

Note that the term \({\int P(D | w') P(w') \text{d}w'} = P(D)\) does not
depend on \(w\) (as the \(w'\) is an integration variable). It is only a
normalisation term. For this reason, we will from this point on write
Bayes' theorem as

\[ P(w | D) = \frac{P(D | w) P(w)}{P(D)}. \]

Bayes' theorem gives us a way of combining data with some ``prior
belief'' on model parameters to obtain a distribution for these model
parameters that considers the data, called the \emph{posterior
distribution}.

    \hypertarget{bayesian-neural-network-with-weight-uncertainty}{%
\subsection{Bayesian neural network with weight
uncertainty}\label{bayesian-neural-network-with-weight-uncertainty}}

The above formula gives a way to determine the distribution of each
weight in the neural network: 1. Pick a prior density \(P(w)\). 2. Using
training data \(D\), determine the likelihood \(P(D | w)\). 3. Determine
the posterior density \(P(w | D)\) using Bayes' theorem. This is the
distribution of the NN weight.

While this works in principle, in many practical settings it is
difficult to implement. The main reason is that the normalisation
constant \({\int P(D | w') P(w') \text{d}w'} = P(D)\) may be very
difficult to calculate, as it involves solving or approximating a
complicated integral. For this reason, approximate methods, such as
\emph{Variational Bayes} described below, are often employed.

    \hypertarget{variational-bayes}{%
\subsection{Variational Bayes}\label{variational-bayes}}

\emph{Variational Bayes} methods approximate the posterior distribution
with a second function, called a \emph{variational posterior}. This
function has a known functional form, and hence avoids the need to
determine the posterior \(P(w | D)\) exactly. Of course, approximating a
function with another one has some risks, since the approximation may be
very bad, leading to a posterior that is highly inaccurate. In order to
mediate this, the variational posterior usually has a number of
parameters, denoted by \(\theta\), that are tuned so that the function
approximates the posterior as well as possible. Let's see how this works
below.

Instead of \(P(w | D)\), we assume the network weight has density
\(q(w | \theta)\), parameterized by \(\theta\). \(q(w | \theta)\) is
known as the \emph{variational posterior}. We want \(q(w | \theta)\) to
approximate \(P(w | D)\), so we want the ``difference'' between
\(q(w | \theta)\) and \(P(w | D)\) to be as small as possible. This
``difference'' between the two distributions is usually measured by the
\href{https://en.wikipedia.org/wiki/Kullback\%E2\%80\%93Leibler_divergence}{Kullback-Leibler
divergence} \(D_{\text{KL}}\) (note that this is \textbf{unrelated} to
the \(D\) we use to denote the data). The Kullback-Leibler divergence
between two distributions with densities \(f(x)\) and \(g(x)\)
respectively is defined as

\[
D_{KL} (f(x) || g(x)) = \int f(x) \log \left( \frac{f(x)}{g(x)} \right) \text{d} x.
\]

Note that this function has value 0 (indicating no difference) when
\(f(x) \equiv g(x)\), which is the result we expect. We use the
convention that \(\frac{0}{0} = 1\) here.

Viewing the data \(D\) as a constant, the Kullback-Leibler divergence
between \(q(w | \theta)\) and \(P(w | D)\) is hence

\[
\begin{align}
D_{KL} (q(w | \theta) || P(w | D)) &= \int q(w | \theta) \log \left( \frac{q(w | \theta)}{P(w | D)} \right) \text{d} w \\
&= \int q(w | \theta) \log \left( \frac{q(w | \theta) P(D)}{P(D | w) P(w)} \right) \text{d} w \\
&= \int q(w | \theta) \log P(D) \text{d} w + \int q(w | \theta) \log \left( \frac{q(w | \theta)}{P(w)} \right) \text{d} w - \int q(w | \theta) \log P(D | w) \text{d} w \\
&= \log P(D) + D_{KL} ( q(w | \theta) || P(w) ) - \mathbb{E}_{q(w | \theta)}(\log P(D | w))
\end{align}
\]

where, in the last line, we have used
\(\int q(w | \theta) \log P(D) \text{d}w = \log P(D) \int q(w | \theta) \text{d} w = \log P(D)\)
since \(q(w | \theta)\) is a probability distribution and hence
integrates to 1. If we consider the data \(D\) to be constant, the first
term is also a constant, and we may ignore it when minimising the above
criterion. Hence, we are left with the function

\[
\begin{align}
L(\theta | D) &= D_{KL} ( q(w | \theta) || P(w) ) - \mathbb{E}_{q(w | \theta)}(\log P(D | w))
\end{align}
\]

Note that this function depends only on \(\theta\) and \(D\), since
\(w\) is an integration variable. This function has a nice
interpretation as the sum of: - The Kullback-Leibler divergence between
the variational posterior \(q(w | \theta)\) and the prior \(P(w)\). This
is called the \emph{complexity cost}, and it depends on \(\theta\) and
the prior \(P(w)\) but not the data \(D\). - The expectation of the
negative loglikelihood \(\log P(D | w)\) under the variational posterior
\(q(w | \theta)\). This is called the \emph{likelihood cost} and it
depends on \(\theta\) and the data \(D\) but not the prior \(P(w)\).

\(L(\theta | D)\) is the loss function that we minimise to determine the
parameter \(\theta\). Note also from the above derivation, that we have

\[
\begin{align}
\log P(D) &= \mathbb{E}_{q(w | \theta)}(\log P(D | w)) - D_{KL} ( q(w | \theta) || P(w) ) + D_{KL} (q(w | \theta) || P(w | D))\\
&\ge \mathbb{E}_{q(w | \theta)}(\log P(D | w)) - D_{KL} ( q(w | \theta) || P(w) ) =: ELBO
\end{align}
\]

which follows because \(D_{KL} (q(w | \theta) || P(w | D))\) is
nonnegative. The final expression on the right hand side is therefore a
lower bound on the log-evidence, and is called the \emph{evidence lower
bound}, often shortened to \emph{ELBO}. The ELBO is the negative of our
loss function, so minimising the loss function is equivalent to
maximising the ELBO.

Maximising the ELBO requires a tradeoff between the KL term and expected
log-likelihood term. On one hand, the divergence between
\(q(w | \theta)\) and \(P(w)\) should be kept small, meaning the
variational posterior shouldn't be too different from the prior. On the
other, the variational posterior parameters should maximise the
expectation of the log-likelihood \(\log P(D | w)\), meaning the model
assigns a high likelihood to the data.

    \hypertarget{backpropagation-scheme}{%
\subsection{Backpropagation scheme}\label{backpropagation-scheme}}

\hypertarget{the-idea}{%
\subsubsection{The idea}\label{the-idea}}

We can use the above ideas to create a neural network with weight
uncertainty, which we will call a \emph{Bayesian neural network}. From a
high level, this works as follows. Suppose we want to determine the
distribution of a particular neural network weight \(w\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assign the weight a prior distribution with density \(P(w)\), which
  represents our beliefs on the possible values of this network before
  any training data. This may be something simple, like a unit Gaussian.
  Furthermore, this prior distribution will usually not have any
  trainable parameters.
\item
  Assign the weight a variational posterior with density
  \(q(w | \theta)\) with some trainable parameter \(\theta\).
\item
  \(q(w | \theta)\) is the approximation for the weight's posterior
  distribution. Tune \(\theta\) to make this approximation as accurate
  as possible as measured by the ELBO.
\end{enumerate}

The remaining question is then how to determine \(\theta\). Recall that
neural networks are typically trained via a backpropagation algorithm,
in which the weights are updated by perturbing them in a direction that
reduces the loss function. We aim to do the same here, by updating
\(\theta\) in a direction that reduces \(L(\theta | D)\).

Hence, the function we want to minimise is

\[
\begin{align}
L(\theta | D) &= D_{KL} ( q(w | \theta) || P(w) ) - \mathbb{E}_{q(w | \theta)}(\log P(D | w)) \\
&= \int q(w | \theta) ( \log q(w | \theta) - \log P(D | w) - \log P(w) ) \text{d}w.
\end{align}
\]

In principle, we could take derivatives of \(L(\theta | D)\) with
respect to \(\theta\) and use this to update its value. However, this
involves doing an integral over \(w\), and this is a calculation that
may be impossible or very computationally expensive. Instead, we want to
write this function as an expectation and use a Monte Carlo
approximation to calculate derivatives. At present, we can write this
function as

\[
\begin{align}
L(\theta | D) &= \mathbb{E}_{q(w | \theta)} ( \log q(w | \theta) - \log P(D | w) - \log P(w) ).
\end{align}
\]

However, taking derivatives with respect to \(\theta\) is difficult
because the underlying distribution the expectation is taken with
respect to depends on \(\theta\). One way we can handle this is with the
\emph{reparameterization trick}.

    \hypertarget{the-reparameterization-trick}{%
\subsubsection{The reparameterization
trick}\label{the-reparameterization-trick}}

The reparameterization trick is a way to move the dependence on
\(\theta\) around so that an expectation may be taken independently of
it. It's easiest to see how this works with an example. Suppose
\(q(w | \theta)\) is a Gaussian, so that \(\theta = (\mu, \sigma)\).
Then, for some arbitrary \(f(w; \mu, \sigma)\), we have

\[
\begin{align}
\mathbb{E}_{q(w | \mu, \sigma)} (f(w; \mu, \sigma) ) &= \int q(w | \mu, \sigma) f(w; \mu, \sigma) \text{d}w \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2 \sigma^2} (w - \mu)^2 \right) f(w; \mu, \sigma) \text{d}w \\
&= \int \frac{1}{\sqrt{2 \pi}} \exp \left( -\frac{1}{2} \epsilon^2 \right) f \left( \mu + \sigma \epsilon; \mu, \sigma \right) \text{d}\epsilon \\
&= \mathbb{E}_{\epsilon \sim N(0, 1)} (f \left( \mu + \sigma \epsilon; \mu, \sigma \right) )
\end{align}
\]

where we used the change of variable \(w = \mu + \sigma \epsilon\). Note
that the dependence on \(\theta = (\mu, \sigma)\) is now only in the
integrand and we can take derivatives with respect to \(\mu\) and
\(\sigma\):

\[
\begin{align}
\frac{\partial}{\partial \mu} \mathbb{E}_{q(w | \mu, \sigma)} (f(w; \mu, \sigma) ) &= \frac{\partial}{\partial \mu} \mathbb{E}_{\epsilon \sim N(0, 1)} (f \left( w; \mu, \sigma \right) ) = \mathbb{E}_{\epsilon \sim N(0, 1)} \frac{\partial}{\partial \mu} f \left( \mu + \sigma \epsilon; \mu, \sigma \right)
\end{align}
\]

\[
\begin{align}
\frac{\partial}{\partial \sigma} \mathbb{E}_{q(w | \mu, \sigma)} (f(w; \mu, \sigma) ) &= \frac{\partial}{\partial \sigma} \mathbb{E}_{\epsilon \sim N(0, 1)} (f \left( w; \mu, \sigma \right) ) = \mathbb{E}_{\epsilon \sim N(0, 1)} \frac{\partial}{\partial \sigma} f \left( \mu + \sigma \epsilon; \mu, \sigma \right)
\end{align}
\]

Finally, note that we can approximate the expectation by its Monte Carlo
estimate:

\[
\begin{align}
\mathbb{E}_{\epsilon \sim N(0, 1)}  \frac{\partial}{\partial \theta} f \left( \mu + \sigma \epsilon; \mu, \sigma \right) \approx \sum_{i}  \frac{\partial}{\partial \theta} f \left( \mu + \sigma \epsilon_i; \mu, \sigma \right),\qquad \epsilon_i \sim N(0, 1).
\end{align}
\]

The above reparameterization trick works in cases where we can write the
\(w = g(\epsilon, \theta)\), where the distribution of the random
variable \(\epsilon\) is independent of \(\theta\).

    \hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

Putting this all together, for our loss function
\(L(\theta | D) \equiv L(\mu, \sigma | D)\), we have

\[
f(w; \mu, \sigma) = \log q(w | \mu, \sigma) - \log P(D | w) - \log P(w)
\]

\[
\begin{align}
\frac{\partial}{\partial \mu} L(\mu, \sigma | D) \approx \sum_{i} \left( \frac{\partial f(w_i; \mu, \sigma)}{\partial w_i} + \frac{\partial f(w_i; \mu, \sigma)}{\partial \mu} \right)
\end{align}
\]

\[
\begin{align}
\frac{\partial}{\partial \sigma} L(\mu, \sigma | D) \approx \sum_{i} \left( \frac{\partial f(w_i; \mu, \sigma)}{\partial w_i} \epsilon_i + \frac{\partial f(w_i; \mu, \sigma)}{\partial \sigma} \right)
\end{align}
\]

where \(w_i = \mu + \sigma \epsilon_i, \, \epsilon_i \sim N(0, 1)\). In
practice, we often only take a single sample \(\epsilon_1\) for each
training point. This leads to the following backpropagation scheme: 1.
Sample \(\epsilon_i \sim N(0, 1)\). 2. Let
\(w_i = \mu + \sigma \epsilon_i\) 3. Calculate \[
\nabla_{\mu}f = \frac{\partial f(w_i; \mu, \sigma)}{\partial w_i} + \frac{\partial f(w_i; \mu, \sigma)}{\partial \mu} \hspace{3em} \nabla_{\sigma}f = \frac{\partial f(w_i; \mu, \sigma)}{\partial w_i} \epsilon_i + \frac{\partial f(w_i; \mu, \sigma)}{\partial \sigma}
\] 4. Update the parameters with some gradient-based optimiser using the
above gradients.

This is how we learn the parameters of the distribution for each neural
network weight.

    \hypertarget{minibatches}{%
\subsubsection{Minibatches}\label{minibatches}}

Note that the loss function (or negative of the ELBO) is

\[
\begin{align}
L(\theta | D) &= D_{KL} ( q(w | \theta) || P(w) ) - \mathbb{E}_{q(w | \theta)}(\log P(D | w)) \\
& = D_{KL} ( q(w | \theta) || P(w) ) - \sum_{j=1}^N \log P(y_j, x_j | w_j)
\end{align}
\]

where \(j\) runs over all the data points in the training data (\(N\) in
total) and \(w_j = \mu + \sigma \epsilon_j\) is sampled using
\(\epsilon_j \sim N(0, 1)\) (we assume a single sample from the
approximate posterior per data point for simplicity).

If training occurs in minibatches of size \(B\), typically much smaller
than \(N\), we instead have a loss function

\[
\begin{align}
L(\theta | D) = D_{KL} ( q(w | \theta) || P(w) ) - \sum_{j=1}^{B} \log P(y_j, x_j | w_j).
\end{align}
\]

Note that the scaling factors between the first and second terms have
changed, since before the sum ran from 1 to \(N\), but it now runs from
1 to \(B\). To correct for this, we should add a correction factor
\(\frac{N}{B}\) to the second term to ensure that its expectation is the
same as before. This leads to the loss function, after dividing by \(N\)
to take the average per training value, of

\[
\begin{align}
L(\theta | D) = \frac{1}{N} D_{KL} ( q(w | \theta) || P(w) ) - \frac{1}{B} \sum_{j=1}^{B} \log P(y_j, x_j | w_j).
\end{align}
\]

Note that by default, when Tensorflow calculates the loss function, it
calculates the average across the minibatch. Hence, it already uses the
factor \(\frac{1}{B}\) present on the second term. However, it does not,
by default, divide the first term by \(N\). In an implementation, we
will have to specify this.

    \hypertarget{conclusion}{%
\subsubsection{Conclusion}\label{conclusion}}

This was a short introduction to \emph{Bayes by Backpropagation} method,
which can be used to embed weight uncertainty into neural networks. This
approach allows the modelling of \emph{epistemic} uncertainty on the
model weights. We expect that, as the number of training samples
increases, the uncertainty on the model weights decreases. This can be
shown to be the case in many settings.

    \hypertarget{further-reading-and-resources}{%
\subsubsection{Further reading and
resources}\label{further-reading-and-resources}}

\begin{itemize}
\tightlist
\item
  Bayes by backprop paper: https://arxiv.org/pdf/1505.05424.pdf
\item
  Wikipedia article on Bayesian inference:
  https://en.wikipedia.org/wiki/Bayesian\_inference
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
